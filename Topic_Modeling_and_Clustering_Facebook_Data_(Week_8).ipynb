{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modeling and Clustering Facebook Data (Week 8)",
      "provenance": [],
      "authorship_tag": "ABX9TyPUpPLAuZlLdbQzmdbDaFVq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaccine-lang/facebook-data/blob/main/Topic_Modeling_and_Clustering_Facebook_Data_(Week_8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0JcZ3pEJtJ4"
      },
      "source": [
        "# Topic Modeling and Clustering Facebook Data\n",
        "\n",
        "![Flounder saying \"This is this and that is that.\"](https://i.imgur.com/fi5fh1C.gif)\n",
        "\n",
        "This week, we will apply topic modeling and clustering to the Facebook data set to see what sorts of \"topics\" emerge. Conceptually, the primary difference between topic modeling and clustering is that topic models allow for overlap: a single document can feature many topics. In clustering, however, a document is assigned to a single cluster. \n",
        "\n",
        "In both cases, however, we set the number of topics/clusters and can adjust for a best fit. Similarly, we can get wordlists of words that are distinct to a specific topic/cluster, which may help with identifying the different \"genres\" of vaccine hesitancy.\n",
        "\n",
        "A good rule of thumb for topics are for there to be a rapid drop off in weights for terms. If the drop off is slow, it suggests an indistinct topic. On the other hand, with clusters you want the clusters similar in size (within an order of magnitude), because otherwise you get a handful of useful clusters and one giant \"misc\" cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLA04sYcDkNs"
      },
      "source": [
        "# Import common libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Import our language libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "\n",
        "# Install and import gensim\n",
        "#!pip install --upgrade gensim\n",
        "\n",
        "# Intsall Levenshtein\n",
        "!pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_104DcPnBPpL"
      },
      "source": [
        " # Import data files from GitHub\n",
        "\n",
        "# Set remote (GitHub) and local paths for the data files\n",
        "GITHUB_ROOT = \"https://raw.githubusercontent.com/vaccine-lang/facebook-data/main\"\n",
        "BASE_DIR = \"/\"\n",
        "print(f'Files will be downloaded from \"{GITHUB_ROOT}\"')\n",
        "print(f'Files will be downloaded to \"{BASE_DIR}\".')\n",
        "\n",
        "# Download the concatinated file\n",
        "file_names = [\"concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only\"]\n",
        "print(\"Downloading data\")\n",
        "for name in file_names:\n",
        "  cmd = \" \".join(['wget', '-P', os.path.dirname(BASE_DIR + name + \".csv\"), GITHUB_ROOT + \"/data/\" + name + \".csv\"])\n",
        "  print(\"!\"+cmd)\n",
        "  if os.system(cmd) != 0:\n",
        "    print('  ~~> ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVh9eXD3hXgP"
      },
      "source": [
        "df = pd.read_csv(\"concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only.csv\")\n",
        "print(len(df)) # check to make sure the number of lines is about right: ~180k.\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tJ3_vsHszBA"
      },
      "source": [
        "The data is imported and converted into a table of ~180k snippets of text. Let's vectorize it, then."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-CaNaJGNlGE"
      },
      "source": [
        "# Let's initialize our vectorizer. Incidentally, this uses the default tokenizer:\n",
        "# r\"(?u)\\b\\w\\w+\\b\"\n",
        "tfidf_text = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
        "tfidf_text_vectors = tfidf_text.fit_transform(df[\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCx5SefHs-8u"
      },
      "source": [
        "## Topic Modeling\n",
        "\n",
        "There are many ways to model topics, and each method also has tweakable parameters. We'll use Non-negative Matrix Factorization (NMF):\n",
        "\n",
        "![W x H ~ V](https://upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png)\n",
        "\n",
        "_V_ is the vector space of our corpus. Each row is a Facebook post, and each column is a word. This is a sparse vector, because typically a word will _not_ appear in a post.\n",
        "\n",
        "_W_ in this example reduces the corpus to two topics (columns) and four posts (columns)\n",
        "\n",
        "_H_, on the other hand, reduces the corpus to two topics (rows) and six words (columns).\n",
        "\n",
        "In other words, the two topics are the matrix that you can multiply the posts and words together to get the original corpus. The topics are like an adapter.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3l7o1noOPzm"
      },
      "source": [
        "# Set some initial values and define a function for displaying topics\n",
        "topics = 10\n",
        "seed = 42\n",
        "def display_topics(model, features, no_top_words=5):\n",
        "  for topic, word_vector in enumerate(model.components_):\n",
        "    total = word_vector.sum()\n",
        "    largest = word_vector.argsort()[::-1] # inverted sort\n",
        "    print(\"\\nTopic %02d\" % topic)\n",
        "    for i in range(0, no_top_words):\n",
        "      print(\" %s (%2.2f)\" % (features[largest[i]], word_vector[largest[i]]*100.0/total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkCRnASVqRo"
      },
      "source": [
        "# Start with NMF topic modeling\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf_text_model = NMF(n_components=topics, random_state=seed)\n",
        "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
        "H_text_matrix = nmf_text_model.components_\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnxeBq-KVq4K"
      },
      "source": [
        "display_topics(nmf_text_model, tfidf_text.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKbLBvtyuQjY"
      },
      "source": [
        "## What should we do with these results?"
      ]
    }
  ]
}