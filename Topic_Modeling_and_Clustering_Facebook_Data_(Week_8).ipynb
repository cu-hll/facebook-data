{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modeling and Clustering Facebook Data (Week 8)",
      "provenance": [],
      "authorship_tag": "ABX9TyNy6wx/3G78jky/qHBVlD9j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaccine-lang/facebook-data/blob/main/Topic_Modeling_and_Clustering_Facebook_Data_(Week_8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0JcZ3pEJtJ4"
      },
      "source": [
        "# Topic Modeling and Clustering Facebook Data\n",
        "\n",
        "This week, we will apply topic modeling and clustering to the Facebook data set to see what sorts of \"topics\" emerge. Conceptually, the primary difference between topic modeling and clustering is that topic models allow for overlap: a single document can feature many topics. In clustering, however, a document is assigned to a single cluster. \n",
        "\n",
        "In both cases, however, we set the number of topics/clusters and can adjust for a best fit. Similarly, we can get wordlists of words that are distinct to a specific topic/cluster, which may help with identifying the different \"genres\" of vaccine hesitancy.\n",
        "\n",
        "A good rule of thumb for topics are for there to be a rapid drop off in weights for terms. If the drop off is slow, it suggests an indistinct topic. On the other hand, with clusters you want the clusters similar in size (within an order of magnitude), because otherwise you get a handful of useful clusters and one giant \"misc\" cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLA04sYcDkNs",
        "outputId": "1872f269-98ac-4d7e-d822-b2889015cd2a"
      },
      "source": [
        "# Import common libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Import our language libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "\n",
        "# Install and import gensim\n",
        "#!pip install --upgrade gensim\n",
        "\n",
        "# Intsall Levenshtein\n",
        "!pip install python-Levenshtein"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 10 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 20 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 30 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 40 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.2.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149865 sha256=d8f69ba81cbdb0adc4901ab6c33606afa041f7491849e58554c5596fd8c1867d\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_104DcPnBPpL",
        "outputId": "b3ab72cb-a2ab-4998-c221-137799219ddd"
      },
      "source": [
        " # Import data files from GitHub\n",
        "\n",
        "# Set remote (GitHub) and local paths for the data files\n",
        "GITHUB_ROOT = \"https://raw.githubusercontent.com/vaccine-lang/facebook-data/main\"\n",
        "BASE_DIR = \"/\"\n",
        "print(f'Files will be downloaded from \"{GITHUB_ROOT}\"')\n",
        "print(f'Files will be downloaded to \"{BASE_DIR}\".')\n",
        "\n",
        "# Download the concatinated file\n",
        "file_names = [\"concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only\"]\n",
        "print(\"Downloading data\")\n",
        "for name in file_names:\n",
        "  cmd = \" \".join(['wget', '-P', os.path.dirname(BASE_DIR + name + \".csv\"), GITHUB_ROOT + \"/data/\" + name + \".csv\"])\n",
        "  print(\"!\"+cmd)\n",
        "  if os.system(cmd) != 0:\n",
        "    print('  ~~> ERROR')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files will be downloaded from \"https://raw.githubusercontent.com/vaccine-lang/facebook-data/main\"\n",
            "Files will be downloaded to \"/\".\n",
            "Downloading data\n",
            "!wget -P / https://raw.githubusercontent.com/vaccine-lang/facebook-data/main/data/concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "qVh9eXD3hXgP",
        "outputId": "febcbce1-54fd-45cf-e23f-18ed79f4affc"
      },
      "source": [
        "df = pd.read_csv(\"concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only.csv\")\n",
        "print(len(df)) # check to make sure the number of lines is about right: ~180k.\n",
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "186822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>#ATTENTION Federal Election is Coming #QUESTIO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Doctors &amp; Nurses are disregarding sound medica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ðŸ¤¬  SouthernDude82 If your still in doubt that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>VICE â€œWhat is being built is the architectur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>WORLDWIDE RALLY FOR FREEDOM [MELBOURNE] On M...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                               text\n",
              "0           0  #ATTENTION Federal Election is Coming #QUESTIO...\n",
              "1           1  Doctors & Nurses are disregarding sound medica...\n",
              "2           2  ðŸ¤¬  SouthernDude82 If your still in doubt that ...\n",
              "3           3    VICE â€œWhat is being built is the architectur...\n",
              "4           4    WORLDWIDE RALLY FOR FREEDOM [MELBOURNE] On M..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-CaNaJGNlGE",
        "outputId": "f0b4182d-e15e-4fc7-fae4-51edb84319b8"
      },
      "source": [
        "tfidf_text = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
        "tfidf_text_vectors = tfidf_text.fit_transform(df[\"text\"])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3l7o1noOPzm"
      },
      "source": [
        "topics = 10\n",
        "seed = 42\n",
        "def display_topics(model, features, no_top_words=5):\n",
        "  for topic, word_vector in enumerate(model.components_):\n",
        "    total = word_vector.sum()\n",
        "    largest = word_vector.argsort()[::-1] # inverted sort\n",
        "    print(\"\\nTopic %02d\" % topic)\n",
        "    for i in range(0, no_top_words):\n",
        "      print(\" %s (%2.2f)\" % (features[largest[i]], word_vector[largest[i]]*100.0/total))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkCRnASVqRo"
      },
      "source": [
        "# Start with NMF topic modeling\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf_text_model = NMF(n_components=topics, random_state=seed)\n",
        "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
        "H_text_matrix = nmf_text_model.components_\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnxeBq-KVq4K",
        "outputId": "02b23879-aa28-44d3-d5b1-30ca0f30aa19"
      },
      "source": [
        "display_topics(nmf_text_model, tfidf_text.get_feature_names())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Topic 00\n",
            " post (37.98)\n",
            " share (37.71)\n",
            " photos (0.81)\n",
            " bodek (0.38)\n",
            " peter (0.30)\n",
            "\n",
            "Topic 01\n",
            " covid (5.25)\n",
            " 19 (4.46)\n",
            " deaths (0.83)\n",
            " cases (0.60)\n",
            " test (0.54)\n",
            "\n",
            "Topic 02\n",
            " https (4.38)\n",
            " com (3.98)\n",
            " www (3.69)\n",
            " youtube (2.01)\n",
            " watch (1.96)\n",
            "\n",
            "Topic 03\n",
            " people (1.08)\n",
            " know (0.58)\n",
            " don (0.54)\n",
            " like (0.47)\n",
            " need (0.43)\n",
            "\n",
            "Topic 04\n",
            " vaccine (5.97)\n",
            " vaccines (1.65)\n",
            " pfizer (0.82)\n",
            " coronavirus (0.71)\n",
            " gates (0.57)\n",
            "\n",
            "Topic 05\n",
            " mask (6.09)\n",
            " masks (4.14)\n",
            " wear (2.53)\n",
            " wearing (2.31)\n",
            " face (1.77)\n",
            "\n",
            "Topic 06\n",
            " trump (6.34)\n",
            " president (3.05)\n",
            " donald (2.34)\n",
            " biden (1.39)\n",
            " election (0.87)\n",
            "\n",
            "Topic 07\n",
            " canada (3.81)\n",
            " trudeau (3.69)\n",
            " justin (1.52)\n",
            " canadians (1.07)\n",
            " government (1.06)\n",
            "\n",
            "Topic 08\n",
            " facebook (11.48)\n",
            " com (5.66)\n",
            " posts (4.57)\n",
            " https (4.42)\n",
            " www (3.91)\n",
            "\n",
            "Topic 09\n",
            " state (0.93)\n",
            " health (0.87)\n",
            " new (0.65)\n",
            " coronavirus (0.59)\n",
            " governor (0.55)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}