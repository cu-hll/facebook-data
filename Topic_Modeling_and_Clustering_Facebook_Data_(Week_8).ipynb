{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modeling and Clustering Facebook Data (Week 8)",
      "provenance": [],
      "authorship_tag": "ABX9TyOvCUCR55k+iE8dDS5eD8Uz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaccine-lang/facebook-data/blob/main/Topic_Modeling_and_Clustering_Facebook_Data_(Week_8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0JcZ3pEJtJ4"
      },
      "source": [
        "# Topic Modeling and Clustering Facebook Data\n",
        "\n",
        "![Flounder saying \"This is this and that is that.\"](https://i.imgur.com/fi5fh1C.gif)\n",
        "\n",
        "This week, we will apply topic modeling and clustering to the Facebook data set to see what sorts of \"topics\" emerge. Conceptually, the primary difference between topic modeling and clustering is that topic models allow for overlap: a single document can feature many topics. In clustering, however, a document is assigned to a single cluster. \n",
        "\n",
        "In both cases, however, we set the number of topics/clusters and can adjust for a best fit. Similarly, we can get wordlists of words that are distinct to a specific topic/cluster, which may help with identifying the different \"genres\" of vaccine hesitancy.\n",
        "\n",
        "A good rule of thumb for topics are for there to be a rapid drop off in weights for terms. If the drop off is slow, it suggests an indistinct topic. On the other hand, with clusters you want the clusters similar in size (within an order of magnitude), because otherwise you get a handful of useful clusters and one giant \"misc\" cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLA04sYcDkNs",
        "outputId": "f7a04325-afd2-47be-8758-48a8b5bb6ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Import common libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Import our language libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "\n",
        "# Install and import gensim\n",
        "#!pip install --upgrade gensim\n",
        "\n",
        "# Intsall Levenshtein\n",
        "!pip install python-Levenshtein"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 10 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 20 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 30 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 40 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.2.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149855 sha256=72bbd4fe4228d6482dbc4d94f78e07eed40b6eae10250951dc69ef460b79c349\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_104DcPnBPpL",
        "outputId": "a159d570-cb8a-402a-cd4e-1013f09d16aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " # Import data files from GitHub\n",
        "\n",
        "# Set remote (GitHub) and local paths for the data files\n",
        "GITHUB_ROOT = \"https://raw.githubusercontent.com/vaccine-lang/facebook-data/main\"\n",
        "BASE_DIR = \"/\"\n",
        "print(f'Files will be downloaded from \"{GITHUB_ROOT}\"')\n",
        "print(f'Files will be downloaded to \"{BASE_DIR}\".')\n",
        "\n",
        "# Download the concatinated file\n",
        "file_names = [\"concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only\"]\n",
        "print(\"Downloading data\")\n",
        "for name in file_names:\n",
        "  cmd = \" \".join(['wget', '-P', os.path.dirname(BASE_DIR + name + \".csv\"), GITHUB_ROOT + \"/data/\" + name + \".csv\"])\n",
        "  print(\"!\"+cmd)\n",
        "  if os.system(cmd) != 0:\n",
        "    print('  ~~> ERROR')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files will be downloaded from \"https://raw.githubusercontent.com/vaccine-lang/facebook-data/main\"\n",
            "Files will be downloaded to \"/\".\n",
            "Downloading data\n",
            "!wget -P / https://raw.githubusercontent.com/vaccine-lang/facebook-data/main/data/concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVh9eXD3hXgP",
        "outputId": "1eb32c63-d75f-42cb-fecd-add0b4cd7dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df = pd.read_csv(\"concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only.csv\")\n",
        "print(len(df)) # check to make sure the number of lines is about right: ~180k.\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "186822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>#ATTENTION Federal Election is Coming #QUESTIO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Doctors &amp; Nurses are disregarding sound medica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ðŸ¤¬  SouthernDude82 If your still in doubt that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>VICE â€œWhat is being built is the architectur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>WORLDWIDE RALLY FOR FREEDOM [MELBOURNE] On M...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                               text\n",
              "0           0  #ATTENTION Federal Election is Coming #QUESTIO...\n",
              "1           1  Doctors & Nurses are disregarding sound medica...\n",
              "2           2  ðŸ¤¬  SouthernDude82 If your still in doubt that ...\n",
              "3           3    VICE â€œWhat is being built is the architectur...\n",
              "4           4    WORLDWIDE RALLY FOR FREEDOM [MELBOURNE] On M..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tJ3_vsHszBA"
      },
      "source": [
        "The data is imported and converted into a table of ~180k snippets of text. Let's vectorize it, then."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srmWsYqi76g2"
      },
      "source": [
        "facebook_stopwords = (\"post\", \"share\", \"like\", \"subscribe\", \"follow\", )\n",
        "# strip out urls\n",
        "# tprep.replace.urls, # replaces all urls with \"_URL_\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-CaNaJGNlGE",
        "outputId": "810a91b4-240d-4beb-af72-e0f909be7c27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's initialize our vectorizer. Incidentally, this uses the default tokenizer:\n",
        "# r\"(?u)\\b\\w\\w+\\b\" \n",
        "tfidf_text = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
        "tfidf_text_vectors = tfidf_text.fit_transform(df[\"text\"])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBRXIA5v4sA9",
        "outputId": "8459650e-edf0-49b5-b304-bedb25d630db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tfidf_text_vectors.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(186822, 45730)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCx5SefHs-8u"
      },
      "source": [
        "## Topic Modeling\n",
        "\n",
        "There are many ways to model topics, and each method also has tweakable parameters. We'll use Non-negative Matrix Factorization (NMF):\n",
        "\n",
        "![W x H ~ V](https://upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png)\n",
        "\n",
        "_V_ is the vector space of our corpus. Each row is a Facebook post, and each column is a word. This is a sparse vector, because typically a word will _not_ appear in a post.\n",
        "\n",
        "_W_ in this example reduces the corpus to two topics (columns) and four posts (columns)\n",
        "\n",
        "_H_, on the other hand, reduces the corpus to two topics (rows) and six words (columns).\n",
        "\n",
        "In other words, the two topics are the matrix that you can multiply the posts and words together to get the original corpus. The topics are like an adapter.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3l7o1noOPzm"
      },
      "source": [
        "# Set some initial values and define a function for displaying topics\n",
        "topics = 10\n",
        "seed = 42\n",
        "def display_topics(model, features, no_top_words=5):\n",
        "  for topic, word_vector in enumerate(model.components_):\n",
        "    total = word_vector.sum()\n",
        "    largest = word_vector.argsort()[::-1] # inverted sort\n",
        "    print(\"\\nTopic %02d\" % topic)\n",
        "    for i in range(0, no_top_words):\n",
        "      print(\" %s (%2.2f)\" % (features[largest[i]], word_vector[largest[i]]*100.0/total), end=\"\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkCRnASVqRo"
      },
      "source": [
        "# Start with NMF topic modeling\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf_text_model = NMF(n_components=topics, random_state=seed)\n",
        "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
        "H_text_matrix = nmf_text_model.components_\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnxeBq-KVq4K",
        "outputId": "87ec08ed-0ade-4100-ac3e-28504da94904",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "display_topics(nmf_text_model, tfidf_text.get_feature_names())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Topic 00\n",
            " post (37.98) share (37.71) photos (0.81) bodek (0.38) peter (0.30)\n",
            "Topic 01\n",
            " covid (5.25) 19 (4.46) deaths (0.83) cases (0.60) test (0.54)\n",
            "Topic 02\n",
            " https (4.38) com (3.98) www (3.69) youtube (2.01) watch (1.96)\n",
            "Topic 03\n",
            " people (1.08) know (0.58) don (0.54) like (0.47) need (0.43)\n",
            "Topic 04\n",
            " vaccine (5.97) vaccines (1.65) pfizer (0.82) coronavirus (0.71) gates (0.57)\n",
            "Topic 05\n",
            " mask (6.09) masks (4.14) wear (2.53) wearing (2.31) face (1.77)\n",
            "Topic 06\n",
            " trump (6.34) president (3.05) donald (2.34) biden (1.39) election (0.87)\n",
            "Topic 07\n",
            " canada (3.81) trudeau (3.69) justin (1.52) canadians (1.07) government (1.06)\n",
            "Topic 08\n",
            " facebook (11.48) com (5.66) posts (4.57) https (4.42) www (3.91)\n",
            "Topic 09\n",
            " state (0.93) health (0.87) new (0.65) coronavirus (0.59) governor (0.55)"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKbLBvtyuQjY"
      },
      "source": [
        "## What should we do with these results?"
      ]
    }
  ]
}