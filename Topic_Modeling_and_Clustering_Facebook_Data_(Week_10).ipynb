{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modeling and Clustering Facebook Data (Week 10)",
      "provenance": [],
      "authorship_tag": "ABX9TyNy7OFU1lX3SHXpZolKkT0Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaccine-lang/facebook-data/blob/main/Topic_Modeling_and_Clustering_Facebook_Data_(Week_10).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUdwK_cWfS-W"
      },
      "source": [
        "# Topic Modeling and Clustering Facebook Data\n",
        "\n",
        "Last week, we tried topic modeling our Facebook data set and decided we should probably do more thinking about how we tokenize our data in order to get more usable topics from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e243VH-bfaOM"
      },
      "source": [
        "# Install textacy\n",
        "!pip install textacy\n",
        "\n",
        "# Import common libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import sys\n",
        "\n",
        "# Import our language libraries\n",
        "import textacy\n",
        "from textacy import preprocessing as tprep\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "\n",
        "# Install and import gensim\n",
        "#!pip install --upgrade gensim\n",
        "\n",
        "# Install Levenshtein\n",
        "#!pip install python-Levenshtein\n",
        "\n",
        "# Install spaCy model\n",
        "!{sys.executable} -m spacy download en_core_web_lg\n",
        "\n",
        "# Import data files from GitHub\n",
        "\n",
        "# Set remote (GitHub) and local paths for the data files\n",
        "GITHUB_ROOT = \"https://raw.githubusercontent.com/vaccine-lang/facebook-data/main\"\n",
        "BASE_DIR = \"/\"\n",
        "print(f'Files will be downloaded from \"{GITHUB_ROOT}\"')\n",
        "print(f'Files will be downloaded to \"{BASE_DIR}\".')\n",
        "\n",
        "# Download the concatinated file\n",
        "file_names = [\"concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only\"]\n",
        "print(\"Downloading data\")\n",
        "for name in file_names:\n",
        "  cmd = \" \".join(['wget', '-P', os.path.dirname(BASE_DIR + name + \".csv\"), GITHUB_ROOT + \"/data/\" + name + \".csv\"])\n",
        "  print(\"!\"+cmd)\n",
        "  if os.system(cmd) != 0:\n",
        "    print('  ~~> ERROR')\n",
        "\n",
        "df = pd.read_csv(\"concatenated_raw_Facebook_data_w_metadata_stripped_out_text_only.csv\").drop(['Unnamed: 0'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp1x9lzSTzuy"
      },
      "source": [
        "## Work with a sample\n",
        "\n",
        "Let's make a sample of the data so we can more easily see what's going on. A sample of 20 posts should catch enough of the edge cases we stumbled upon last week."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgTOUqAUUBZJ"
      },
      "source": [
        "s = df.sample(20, random_state=8)\n",
        "print(s.to_markdown())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwI_b7MQeML3"
      },
      "source": [
        "Things are looking good, but we can see, again, the prevalence of non-word things in the texts that would nevertheless carry some sort of semantic meaning. In fact, let's see how we can use tokenization here to understand the data better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BEh2qPQegH2"
      },
      "source": [
        "## Tokenizing\n",
        "\n",
        "The simplest tokenizer for English is just to split tokens based on spaces. This may also help us understand what \"non-English\" tokens there are. So let's create an array of everything in the dataset separated by spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4KfEWbMe0nz"
      },
      "source": [
        "# Create a new column for space-split tokens.\n",
        "df[\"naive_tokens\"] = df[\"text\"].str.split(\" \")\n",
        "# Collapse the Series of lists into a giant Series and then get unique values.\n",
        "unique_tokens = pd.unique(df[\"naive_tokens\"].explode())\n",
        "nonword_tokens = [token for token in unique_tokens if re.search(\"^[^a-zA-Z]+$\", token)]\n",
        "print(len(nonword_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtOuPDhYk8AW"
      },
      "source": [
        "# Most of the nonword tokens look like numbers, so let's remove all of the tokens thaat are just numbers with some other special characters\n",
        "nonword_nonnumber_tokens = [non_number for non_number in nonword_tokens if re.search(\"^[^0-9:/.,$]+$\", non_number)]\n",
        "print(len(nonword_nonnumber_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKejA_k0lfRh"
      },
      "source": [
        "# What kinds of tokens remain?\n",
        "nonword_nonnumber_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Nt1l9QqMUc"
      },
      "source": [
        "Because of how we established our filters, we're still catching words in this net, but they aren't English words. Furthermore, we see how much of the non-word/non-number tokens are emoji. Some combination of dropping non-English words and emoji would be helpful. Here's where Textacy can help, especially with its url processing as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVIu_HnhqXaX"
      },
      "source": [
        "def process_emoji(emoji):\n",
        "  text = unicodedata.name(emoji).replace(\" \", \"_\").lower()\n",
        "  return text + \"_emoji \"\n",
        "\n",
        "def process_url(url):\n",
        "  domain = url.lower().split(\"/\")[2].split(\".\")[-2]\n",
        "  return domain + \"_url \"\n",
        "\n",
        "preproc = tprep.make_pipeline(\n",
        "    tprep.normalize.unicode,\n",
        "    tprep.normalize.whitespace,\n",
        "    tprep.normalize.quotation_marks,\n",
        "    tprep.replace.phone_numbers\n",
        ")\n",
        "\n",
        "def process_text(text):\n",
        "  processed_text = preproc(text)\n",
        "  processed_text = tprep.normalize.repeating_chars(processed_text, chars=\"!\")\n",
        "  processed_text = tprep.replace.emojis(processed_text, lambda reMatch: process_emoji(reMatch[0]))\n",
        "  processed_text = tprep.replace.urls(processed_text, lambda reMatch: process_url(reMatch[0]))\n",
        "  return processed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D83n56DrtRq7"
      },
      "source": [
        "s[\"processed\"] = s.apply(lambda row: process_text(row[\"text\"]), axis=1)\n",
        "print(s[\"processed\"].to_markdown())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxsHDlM0iKkz"
      },
      "source": [
        "\n",
        "\n",
        "Let's try to use a more robust tokenizer than we used [last time](https://colab.research.google.com/github/vaccine-lang/facebook-data/blob/main/Topic_Modeling_and_Clustering_Facebook_Data_(Week_8).ipynb). \n",
        "\n",
        "So far, we haven't used any tokenizer; Textacy's preprocessing functions all worked with regular expressions and the like. But now we have to get more robust tools."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftmg0r3edmyA"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdcAhRuXft3k"
      },
      "source": [
        "s[\"doc\"] = s.apply(lambda row: nlp(row[\"processed\"]), axis=1)\n",
        "print(s[\"doc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w860Tvzsjv9U"
      },
      "source": [
        "## Vectorizing\n",
        "\n",
        "Next, let's look at Textacy's [vectorizer](https://textacy.readthedocs.io/en/latest/api_reference/representations.html#vectorizers) and see about how we can build our vectors. We used the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) last time, so let's compare our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJuKAH3XgtFJ"
      },
      "source": [
        "# scikit-learn Incidentally, this uses the default tokenizer:\n",
        "# r\"(?u)\\b\\w\\w+\\b\" \n",
        "tfidf_text = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
        "tfidf_text_vectors = tfidf_text.fit_transform(df[\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAMqI1UBPPiz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}